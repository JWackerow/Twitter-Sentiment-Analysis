{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "by8FWYEEmHb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ccb74f4-527c-43a3-e612-fcad30523e07"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Embedding, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Flatten\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import TweetTokenizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "\r\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "import xgboost as xgb\r\n",
        "\r\n",
        "from gensim.utils import simple_preprocess\r\n",
        "\r\n",
        "import re\r\n",
        "\r\n",
        "import matplotlib as plt\r\n",
        "\r\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D00SchI8qu-B",
        "outputId": "e13a568b-d3be-42fe-fc86-c2b553940fc9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5cfA01Wq8xO"
      },
      "source": [
        "data = pd.read_csv(r'/content/gdrive/My Drive/Datasets/Tweets.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEDmgicftvM6",
        "outputId": "e4a501ce-57e7-46df-da3a-e4d2dd723963"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "odK2kVMItWHN",
        "outputId": "c51a0edc-7d52-4e7c-8819-a1259fd6e9ae"
      },
      "source": [
        "data.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[2 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vxj7xnAstdpH",
        "outputId": "5a4c56f0-9e87-48c2-9f63-ae0a16a418df"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14640 entries, 0 to 14639\n",
            "Data columns (total 15 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   tweet_id                      14640 non-null  int64  \n",
            " 1   airline_sentiment             14640 non-null  object \n",
            " 2   airline_sentiment_confidence  14640 non-null  float64\n",
            " 3   negativereason                9178 non-null   object \n",
            " 4   negativereason_confidence     10522 non-null  float64\n",
            " 5   airline                       14640 non-null  object \n",
            " 6   airline_sentiment_gold        40 non-null     object \n",
            " 7   name                          14640 non-null  object \n",
            " 8   negativereason_gold           32 non-null     object \n",
            " 9   retweet_count                 14640 non-null  int64  \n",
            " 10  text                          14640 non-null  object \n",
            " 11  tweet_coord                   1019 non-null   object \n",
            " 12  tweet_created                 14640 non-null  object \n",
            " 13  tweet_location                9907 non-null   object \n",
            " 14  user_timezone                 9820 non-null   object \n",
            "dtypes: float64(2), int64(2), object(11)\n",
            "memory usage: 1.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYcH7zrR5b3V"
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\r\n",
        "stemmer = PorterStemmer()\r\n",
        "\r\n",
        "def preprocess_text(doc, remove_handles=False, remove_stopwords=False, stop_words=list(), stem=False):\r\n",
        "  \r\n",
        "  if remove_handles:\r\n",
        "    doc = re.sub(\"[@](\\w|\\d[_])+\", \"\", doc, flags=re.I)\r\n",
        "  # format numbers so they all match. To avoid being confused as hashtags, I use 'num' instead of #\r\n",
        "  doc = re.sub('\\d+', \"num\", doc)\r\n",
        "  # use gensim's simple_preprocess to clean up text\r\n",
        "  doc = simple_preprocess(doc=doc, deacc=True)\r\n",
        "\r\n",
        "  if remove_stopwords or stem:\r\n",
        "    doc_hold = list()\r\n",
        "    for token in doc:\r\n",
        "      if remove_stopwords:\r\n",
        "        if token not in stop_words:\r\n",
        "          if stem:\r\n",
        "            doc_hold.append(stemmer.stem(token))\r\n",
        "      elif stem:\r\n",
        "        doc_hold.append(stemmer.stem(token))\r\n",
        "      else:\r\n",
        "        doc_hold.append(token)\r\n",
        "\r\n",
        "    # if the entire tweet happened to be stop words, replace with \"stopword\" token\r\n",
        "    if len(doc_hold) == 0:\r\n",
        "      doc_hold.append(\"stopword\")\r\n",
        "    doc = ' '.join(doc_hold)\r\n",
        "  \r\n",
        "  return doc"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WJZAd084iB3"
      },
      "source": [
        "# clean up/preprocess tweet text\r\n",
        "texts = data.text.apply(lambda x: preprocess_text(x, remove_handles=True, remove_stopwords=False, stop_words=stop_words, stem=True))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX1YijH9FT6N",
        "outputId": "2a4e91b3-4d97-47bc-dbe8-2312d2a7a4ba"
      },
      "source": [
        "texts[:5]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                            what said\n",
              "1           plu you ve ad commerci to the experi tacki\n",
              "2         didn today must mean need to take anoth trip\n",
              "3    it realli aggress to blast obnoxi entertain in...\n",
              "4                 and it realli big bad thing about it\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPXrpCFl-aE-"
      },
      "source": [
        "# Set max vocab size, oov-token, and fit tokenizer\r\n",
        "max_vocab_size = 10000\r\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', oov_token='OOV')\r\n",
        "tokenizer.fit_on_texts(texts)\r\n",
        "\r\n",
        "# Set max sequence length, these are tweets so they can be short\r\n",
        "max_seq_len = 60\r\n",
        "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=max_seq_len, padding='post')\r\n",
        "y = data.airline_sentiment\r\n",
        "\r\n",
        "# encode the target (sentiment) labels\r\n",
        "le = LabelEncoder()\r\n",
        "y = le.fit_transform(y)\r\n",
        "\r\n",
        "# Split data into training and testing\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LtUH5rvGWEe"
      },
      "source": [
        "**Use grid search to find best number of filters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apM6emVA_Ri9"
      },
      "source": [
        "def create_model_filters(filters=256):\r\n",
        "  input_dim = len(tokenizer.word_index) + 1\r\n",
        "  model = Sequential([\r\n",
        "                        Embedding(input_dim=input_dim, output_dim=128),\r\n",
        "                        Conv1D(filters, 5),\r\n",
        "                        GlobalMaxPooling1D(),\r\n",
        "                        Flatten(),\r\n",
        "                        Dropout(.4),\r\n",
        "                        Dense(64, activation='relu'),\r\n",
        "                        Dropout(.4),\r\n",
        "                        Dense(8, activation='relu'),\r\n",
        "                        Dropout(.4),\r\n",
        "                        Dense(3, activation='softmax')])\r\n",
        "\r\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzyU0CMRmyl0"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model_filters, epochs=5)\r\n",
        "param_grid = {'filters' : [ 256, 512]}\r\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='f1_micro', n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L6HdZjaAVOw",
        "outputId": "9a07e800-3c30-450b-eb4e-e3b63fcc5303"
      },
      "source": [
        "history = grid.fit(X=X, y=y)\r\n",
        "print(grid.best_params_)\r\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "458/458 [==============================] - 31s 67ms/step - loss: 0.8908 - accuracy: 0.5948\n",
            "Epoch 2/5\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.5764 - accuracy: 0.7760\n",
            "Epoch 3/5\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.4648 - accuracy: 0.8285\n",
            "Epoch 4/5\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.3753 - accuracy: 0.8568\n",
            "Epoch 5/5\n",
            "458/458 [==============================] - 30s 66ms/step - loss: 0.3015 - accuracy: 0.8873\n",
            "{'filters': 256}\n",
            "GridSearchCV(cv=None, error_score=nan,\n",
            "             estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7f6901bfe0b8>,\n",
            "             iid='deprecated', n_jobs=-1, param_grid={'filters': [256, 512]},\n",
            "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
            "             scoring='f1_micro', verbose=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDOMLdiDGe4x"
      },
      "source": [
        "**Use grid search to find best kernel size**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "096VjUMvF3Az"
      },
      "source": [
        "def create_model_kernel_size(kernel_size=5):\r\n",
        "  input_dim = len(tokenizer.word_index) + 1\r\n",
        "  model = Sequential([\r\n",
        "                        Embedding(input_dim=input_dim, output_dim=128),\r\n",
        "                        Conv1D(256, kernel_size),\r\n",
        "                        GlobalMaxPooling1D(),\r\n",
        "                        Flatten(),\r\n",
        "                        Dropout(.4),\r\n",
        "                        Dense(64, activation='relu'),\r\n",
        "                        Dropout(.4),\r\n",
        "                        Dense(8, activation='relu'),\r\n",
        "                        Dropout(.4),\r\n",
        "                        Dense(3, activation='softmax')])\r\n",
        "\r\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O5ePFhCF3FL"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model_kernel_size, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEoJKCz2GK8v"
      },
      "source": [
        "param_grid = {'kernel_size' : [ 4, 5, 6]}\r\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='f1_micro', n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHIt2tcsGLDA",
        "outputId": "720b16f0-0bb3-457d-d657-297d858853a8"
      },
      "source": [
        "history = grid.fit(X=X, y=y)\r\n",
        "print(grid.best_params_)\r\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "458/458 [==============================] - 31s 67ms/step - loss: 0.8570 - accuracy: 0.6259\n",
            "Epoch 2/5\n",
            "458/458 [==============================] - 31s 67ms/step - loss: 0.5733 - accuracy: 0.7675\n",
            "Epoch 3/5\n",
            "458/458 [==============================] - 31s 67ms/step - loss: 0.4539 - accuracy: 0.8139\n",
            "Epoch 4/5\n",
            "458/458 [==============================] - 32s 69ms/step - loss: 0.3528 - accuracy: 0.8511\n",
            "Epoch 5/5\n",
            "458/458 [==============================] - 31s 67ms/step - loss: 0.2955 - accuracy: 0.8713\n",
            "{'kernel_size': 5}\n",
            "GridSearchCV(cv=None, error_score=nan,\n",
            "             estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7feada5f28d0>,\n",
            "             iid='deprecated', n_jobs=-1, param_grid={'kernel_size': [4, 5, 6]},\n",
            "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
            "             scoring='f1_micro', verbose=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhIttSI4E4BO"
      },
      "source": [
        "**Use grid search to find best dropout**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bguCNGx1HSAa"
      },
      "source": [
        "def create_model_dropout(rate=.2):\r\n",
        "  input_dim = len(tokenizer.word_index) + 1\r\n",
        "  model = Sequential([\r\n",
        "                        Embedding(input_dim=input_dim, output_dim=128),\r\n",
        "                        Conv1D(256, 5),\r\n",
        "                        GlobalMaxPooling1D(),\r\n",
        "                        Flatten(),\r\n",
        "                        Dropout(rate),\r\n",
        "                        Dense(64, activation='relu'),\r\n",
        "                        Dropout(rate),\r\n",
        "                        Dense(8, activation='relu'),\r\n",
        "                        Dropout(rate),\r\n",
        "                        Dense(3, activation='softmax')])\r\n",
        "\r\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5_SjMKaHSVj"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model_dropout, epochs=5)\r\n",
        "param_grid = {'rate':[.2, .3, .4, .5]}\r\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='f1_micro', n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2QWyyB4HSaT"
      },
      "source": [
        "history = grid.fit(X, y)\r\n",
        "print(grid.best_params_)\r\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yj18ndqHSey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59757d9-e028-411d-d9a6-8b6d5547a3db"
      },
      "source": [
        "print(grid.best_params_)\r\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rate': 0.5}\n",
            "GridSearchCV(cv=None, error_score=nan,\n",
            "             estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7f672fc13ef0>,\n",
            "             iid='deprecated', n_jobs=-1,\n",
            "             param_grid={'rate': [0.2, 0.3, 0.4, 0.5]}, pre_dispatch='2*n_jobs',\n",
            "             refit=True, return_train_score=False, scoring='f1_micro',\n",
            "             verbose=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H27vaAGHTNo"
      },
      "source": [
        "**Final Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTF8dnxtIOb"
      },
      "source": [
        "input_dim = len(tokenizer.word_index) + 1\r\n",
        "model = Sequential([\r\n",
        "                        Embedding(input_dim=input_dim, output_dim=128),\r\n",
        "                        Conv1D(256, 5),\r\n",
        "                        GlobalMaxPooling1D(),\r\n",
        "                        Flatten(),\r\n",
        "                        Dropout(.5),\r\n",
        "                        Dense(64, activation='relu'),\r\n",
        "                        Dropout(.5),\r\n",
        "                        Dense(8, activation='relu'),\r\n",
        "                        Dropout(.5),\r\n",
        "                        Dense(3, activation='softmax')\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfN9BylbFHO-",
        "outputId": "18aef1ee-0819-445d-8511-f78743910bd5"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=64, epochs=15)\r\n",
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\r\n",
        "print(f1_score(y_test, y_pred, average='micro'))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 0.9455 - accuracy: 0.6047\n",
            "Epoch 2/15\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 0.7200 - accuracy: 0.6852\n",
            "Epoch 3/15\n",
            "183/183 [==============================] - 19s 106ms/step - loss: 0.6020 - accuracy: 0.7347\n",
            "Epoch 4/15\n",
            "183/183 [==============================] - 19s 105ms/step - loss: 0.5236 - accuracy: 0.7795\n",
            "Epoch 5/15\n",
            "183/183 [==============================] - 19s 106ms/step - loss: 0.4375 - accuracy: 0.8101\n",
            "Epoch 6/15\n",
            "183/183 [==============================] - 20s 107ms/step - loss: 0.3985 - accuracy: 0.8389\n",
            "Epoch 7/15\n",
            "183/183 [==============================] - 20s 107ms/step - loss: 0.3514 - accuracy: 0.8543\n",
            "Epoch 8/15\n",
            "183/183 [==============================] - 20s 107ms/step - loss: 0.3221 - accuracy: 0.8752\n",
            "Epoch 9/15\n",
            "183/183 [==============================] - 20s 107ms/step - loss: 0.3021 - accuracy: 0.8717\n",
            "Epoch 10/15\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 0.2912 - accuracy: 0.8804\n",
            "Epoch 11/15\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 0.2686 - accuracy: 0.8865\n",
            "Epoch 12/15\n",
            "183/183 [==============================] - 20s 108ms/step - loss: 0.2299 - accuracy: 0.9044\n",
            "Epoch 13/15\n",
            "183/183 [==============================] - 20s 109ms/step - loss: 0.2313 - accuracy: 0.9032\n",
            "Epoch 14/15\n",
            "183/183 [==============================] - 20s 109ms/step - loss: 0.2230 - accuracy: 0.9053\n",
            "Epoch 15/15\n",
            "183/183 [==============================] - 20s 107ms/step - loss: 0.2194 - accuracy: 0.9085\n",
            "0.7503415300546448\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}