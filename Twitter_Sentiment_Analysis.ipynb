{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "by8FWYEEmHb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abaecd1e-4a7c-4e56-ec26-6b58553db6a1"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Embedding, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Flatten\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "\r\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "\r\n",
        "from gensim.utils import simple_preprocess\r\n",
        "\r\n",
        "import re\r\n",
        "\r\n",
        "import matplotlib as plt\r\n",
        "\r\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D00SchI8qu-B",
        "outputId": "08542f1b-d04a-41ff-f0a3-ef6179291ec1"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5cfA01Wq8xO"
      },
      "source": [
        "data = pd.read_csv(r'/content/gdrive/My Drive/Datasets/Tweets.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEDmgicftvM6",
        "outputId": "de5e934e-f87c-4556-deb9-8969935d9c70"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "odK2kVMItWHN",
        "outputId": "69728f1e-26a6-4015-fe87-b1aa4142c486"
      },
      "source": [
        "data.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[2 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vxj7xnAstdpH",
        "outputId": "029bb778-d7f3-484b-bb3e-945f03239bc6"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14640 entries, 0 to 14639\n",
            "Data columns (total 15 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   tweet_id                      14640 non-null  int64  \n",
            " 1   airline_sentiment             14640 non-null  object \n",
            " 2   airline_sentiment_confidence  14640 non-null  float64\n",
            " 3   negativereason                9178 non-null   object \n",
            " 4   negativereason_confidence     10522 non-null  float64\n",
            " 5   airline                       14640 non-null  object \n",
            " 6   airline_sentiment_gold        40 non-null     object \n",
            " 7   name                          14640 non-null  object \n",
            " 8   negativereason_gold           32 non-null     object \n",
            " 9   retweet_count                 14640 non-null  int64  \n",
            " 10  text                          14640 non-null  object \n",
            " 11  tweet_coord                   1019 non-null   object \n",
            " 12  tweet_created                 14640 non-null  object \n",
            " 13  tweet_location                9907 non-null   object \n",
            " 14  user_timezone                 9820 non-null   object \n",
            "dtypes: float64(2), int64(2), object(11)\n",
            "memory usage: 1.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYcH7zrR5b3V"
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\r\n",
        "stemmer = PorterStemmer()\r\n",
        "\r\n",
        "def preprocess_text(doc, remove_handles=True, remove_stopwords=False, stop_words=list(), stem=True):\r\n",
        "  \r\n",
        "  if remove_handles:\r\n",
        "    doc = re.sub(\"[@](\\w|\\d[_])+\", \"\", doc, flags=re.I)\r\n",
        "  # format numbers so they all match. To avoid being confused as hashtags, I use 'num' instead of #\r\n",
        "  doc = re.sub('\\d+', \"num\", doc)\r\n",
        "  # use gensim's simple_preprocess to clean up text\r\n",
        "  doc = simple_preprocess(doc=doc, deacc=True)\r\n",
        "\r\n",
        "  if remove_stopwords or stem:\r\n",
        "    doc_hold = list()\r\n",
        "    for token in doc:\r\n",
        "      if remove_stopwords:\r\n",
        "        if token not in stop_words:\r\n",
        "          if stem:\r\n",
        "            doc_hold.append(stemmer.stem(token))\r\n",
        "      elif stem:\r\n",
        "        doc_hold.append(stemmer.stem(token))\r\n",
        "      else:\r\n",
        "        doc_hold.append(token)\r\n",
        "\r\n",
        "    # If the entire tweet happened to be stop words, replace with \"stopword\" token\r\n",
        "    if len(doc_hold) == 0:\r\n",
        "      doc_hold.append(\"stopword\")\r\n",
        "    doc = ' '.join(doc_hold)\r\n",
        "  \r\n",
        "  return doc"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WJZAd084iB3"
      },
      "source": [
        "# Clean up/preprocess tweet text\r\n",
        "texts = data.text.apply(lambda x: preprocess_text(x, remove_handles=True, remove_stopwords=False, stop_words=stop_words, stem=True))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX1YijH9FT6N",
        "outputId": "43c5f3ee-0bf6-4191-adc6-b2997251dc74"
      },
      "source": [
        "texts[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                            what said\n",
              "1           plu you ve ad commerci to the experi tacki\n",
              "2         didn today must mean need to take anoth trip\n",
              "3    it realli aggress to blast obnoxi entertain in...\n",
              "4                 and it realli big bad thing about it\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPXrpCFl-aE-"
      },
      "source": [
        "# set y and encode labels\r\n",
        "y = data.airline_sentiment\r\n",
        "# Encode the target (sentiment) labels\r\n",
        "le = LabelEncoder()\r\n",
        "y = le.fit_transform(y)\r\n",
        "\r\n",
        "# Split data into training and testing\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=.2, random_state=1)\r\n",
        "\r\n",
        "# Set max vocab size, oov-token, and fit tokenizer\r\n",
        "max_vocab_size = 10000\r\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', oov_token='OOV')\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "\r\n",
        "# Set max sequence length, these are tweets so it can be short\r\n",
        "max_seq_len = 50\r\n",
        "\r\n",
        "# Convert training and test data to sequences\r\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_seq_len, padding='post')\r\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_seq_len, padding='post')\r\n",
        "\r\n",
        "# Convert all text to sequences to us for grid search hyperparameter tuning\r\n",
        "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=max_seq_len, padding='post')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LtUH5rvGWEe"
      },
      "source": [
        "**Use grid search to tune hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apM6emVA_Ri9"
      },
      "source": [
        "def create_model(filters=256, kernel_size=5, rate=.2):\r\n",
        "  input_dim = len(tokenizer.word_index) + 1\r\n",
        "  model = Sequential([\r\n",
        "                        Embedding(input_dim=input_dim, output_dim=128),\r\n",
        "                        Conv1D(filters, kernel_size),\r\n",
        "                        GlobalMaxPooling1D(),\r\n",
        "                        Flatten(),\r\n",
        "                        Dropout(rate),\r\n",
        "                        Dense(64, activation='relu'),\r\n",
        "                        Dropout(rate),\r\n",
        "                        Dense(8, activation='relu'),\r\n",
        "                        Dropout(rate),\r\n",
        "                        Dense(3, activation='softmax')])\r\n",
        "\r\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n",
        "  return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzyU0CMRmyl0"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, epochs=5)\r\n",
        "param_grid = {'filters' : [ 256, 512], 'kernel_size' : [ 3, 4, 5, 6], 'rate' : [.2, .3, .4, .5] }\r\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='f1_micro', n_jobs=-1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L6HdZjaAVOw",
        "outputId": "90422a23-6b78-4e07-f16b-b913dc30b240"
      },
      "source": [
        "history = grid.fit(X=X, y=y)\r\n",
        "print(grid.best_params_)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "458/458 [==============================] - 13s 27ms/step - loss: 0.7958 - accuracy: 0.6621\n",
            "Epoch 2/5\n",
            "458/458 [==============================] - 13s 27ms/step - loss: 0.4750 - accuracy: 0.8211\n",
            "Epoch 3/5\n",
            "458/458 [==============================] - 13s 28ms/step - loss: 0.3438 - accuracy: 0.8771\n",
            "Epoch 4/5\n",
            "458/458 [==============================] - 13s 28ms/step - loss: 0.2271 - accuracy: 0.9202\n",
            "Epoch 5/5\n",
            "458/458 [==============================] - 12s 27ms/step - loss: 0.1523 - accuracy: 0.9437\n",
            "{'filters': 256, 'kernel_size': 3, 'rate': 0.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H27vaAGHTNo"
      },
      "source": [
        "**Final Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTF8dnxtIOb"
      },
      "source": [
        "input_dim = len(tokenizer.word_index) + 1\r\n",
        "model = Sequential([\r\n",
        "                        Embedding(input_dim=input_dim, output_dim=128),\r\n",
        "                        Conv1D(256, 3),\r\n",
        "                        GlobalMaxPooling1D(),\r\n",
        "                        Flatten(),\r\n",
        "                        Dropout(.2),\r\n",
        "                        Dense(64, activation='relu'),\r\n",
        "                        Dropout(.2),\r\n",
        "                        Dense(8, activation='relu'),\r\n",
        "                        Dropout(.2),\r\n",
        "                        Dense(3, activation='softmax')\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfN9BylbFHO-",
        "outputId": "b96d9d70-a323-4cd5-c10d-164cb27a4eee"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=64, epochs=15)\r\n",
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\r\n",
        "print(f1_score(y_test, y_pred, average='micro'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "183/183 [==============================] - 9s 44ms/step - loss: 0.8729 - accuracy: 0.6356\n",
            "Epoch 2/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.5415 - accuracy: 0.7889\n",
            "Epoch 3/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.4016 - accuracy: 0.8454\n",
            "Epoch 4/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.2633 - accuracy: 0.9046\n",
            "Epoch 5/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.1927 - accuracy: 0.9298\n",
            "Epoch 6/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.1338 - accuracy: 0.9533\n",
            "Epoch 7/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.1009 - accuracy: 0.9624\n",
            "Epoch 8/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.0933 - accuracy: 0.9652\n",
            "Epoch 9/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.0709 - accuracy: 0.9748\n",
            "Epoch 10/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.0740 - accuracy: 0.9745\n",
            "Epoch 11/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.0557 - accuracy: 0.9810\n",
            "Epoch 12/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.0475 - accuracy: 0.9826\n",
            "Epoch 13/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.0571 - accuracy: 0.9778\n",
            "Epoch 14/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.0576 - accuracy: 0.9804\n",
            "Epoch 15/15\n",
            "183/183 [==============================] - 8s 44ms/step - loss: 0.0537 - accuracy: 0.9798\n",
            "0.7520491803278688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkHC3tSSkgGa"
      },
      "source": [
        "**OneVsRest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu8c9QyZnLhG",
        "outputId": "81095cfe-079b-49ea-9815-a53007d4cce4"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# Create pipeline for preprocessing and classifier\r\n",
        "pipe = Pipeline(steps=([('tfidf', TfidfVectorizer(strip_accents='unicode')), ('classifier', OneVsRestClassifier(LogisticRegression(max_iter=250, random_state=0)))]))\r\n",
        "# Use grid search to optimize best hyperparameters\r\n",
        "param_grid = {'classifier__estimator__C': [2, 3, 4, 5, 6], 'classifier__estimator__penalty' : ['l1', 'l2']}\r\n",
        "grid = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='f1_micro', n_jobs=-1)\r\n",
        "grid.fit(data.text, y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('tfidf',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_...\n",
              "                                                                                         multi_class='auto',\n",
              "                                                                                         n_jobs=None,\n",
              "                                                                                         penalty='l2',\n",
              "                                                                                         random_state=0,\n",
              "                                                                                         solver='lbfgs',\n",
              "                                                                                         tol=0.0001,\n",
              "                                                                                         verbose=0,\n",
              "                                                                                         warm_start=False),\n",
              "                                                            n_jobs=None))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'classifier__estimator__C': [2, 3, 4, 5, 6],\n",
              "                         'classifier__estimator__penalty': ['l1', 'l2']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1_micro', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F_c7emGnmm1",
        "outputId": "131278d8-f0af-46cc-9a28-240d8c00179f"
      },
      "source": [
        "# Print best parameters\r\n",
        "print(grid.best_params_)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'classifier__estimator__C': 4, 'classifier__estimator__penalty': 'l2'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK4QkPgRoQvQ",
        "outputId": "694f561b-54db-4d10-c64b-a38c9fcc422c"
      },
      "source": [
        "# Split texts into training and testing again since we are using the TdidftVectorizer this time\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=.2, random_state=1)\r\n",
        "\r\n",
        "# build and train model with best hyperparameters\r\n",
        "model = Pipeline(steps=([('tfidf', TfidfVectorizer(strip_accents='unicode')), ('classifier', OneVsRestClassifier(LogisticRegression(C=4, max_iter=250, random_state=0)))]))\r\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents='unicode',\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patt...\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 OneVsRestClassifier(estimator=LogisticRegression(C=4,\n",
              "                                                                  class_weight=None,\n",
              "                                                                  dual=False,\n",
              "                                                                  fit_intercept=True,\n",
              "                                                                  intercept_scaling=1,\n",
              "                                                                  l1_ratio=None,\n",
              "                                                                  max_iter=250,\n",
              "                                                                  multi_class='auto',\n",
              "                                                                  n_jobs=None,\n",
              "                                                                  penalty='l2',\n",
              "                                                                  random_state=0,\n",
              "                                                                  solver='lbfgs',\n",
              "                                                                  tol=0.0001,\n",
              "                                                                  verbose=0,\n",
              "                                                                  warm_start=False),\n",
              "                                     n_jobs=None))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IStdEzcHm8lZ",
        "outputId": "98908eac-d87d-4867-b354-2df9ea836dbd"
      },
      "source": [
        "# Test accuracy\r\n",
        "y_pred = model.predict(X_test)\r\n",
        "f1_score(y_test, y_pred, average='micro')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7995218579234973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}